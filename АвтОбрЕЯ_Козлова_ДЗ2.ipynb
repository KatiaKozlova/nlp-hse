{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058a281a46f3482e872264411126fd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading https://raw.githubusercontent.com/stanfordnlp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 17:10:25 INFO: Downloading default packages for language: ru (Russian) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c416a9c202483990b776a357a1f706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading https://huggingface.co/stanfordnlp/stanza-ru/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 17:11:42 INFO: Finished downloading models and saved to C:\\Users\\katia\\stanza_resources.\n",
      "2022-10-13 17:11:42 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0be047e28234baab1f41532b18509ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading https://raw.githubusercontent.com/stanfordnlp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 17:11:46 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
      "2022-10-13 17:11:46 INFO: Use device: cpu\n",
      "2022-10-13 17:11:46 INFO: Loading: tokenize\n",
      "2022-10-13 17:11:46 INFO: Loading: pos\n",
      "2022-10-13 17:11:47 INFO: Loading: lemma\n",
      "2022-10-13 17:11:47 INFO: Loading: depparse\n",
      "2022-10-13 17:11:47 INFO: Loading: ner\n",
      "2022-10-13 17:11:50 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import csv\n",
    "import stanza\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline('ru')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation += '—' + '»' + '«'\n",
    "punctuation = punctuation.split('-')[0] + punctuation.split('-')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание, разметка корпуса и объяснение того, почему этот текст подходит для оценки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я выбрала цикл «лингвистических сказок» Л. Петрушевской \"Пуськи бятые\", так все неслужебные слова в нем полностью выдуманы, но сохраняют некоторые признаки изначальных частей речи (окончания, суффиксы). Таким образом мы можем проверить, насколько автоматический теггер справится с синтаксически верными последовательностями ненастоящих слов (авторских неологизмов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Какой тегсет я беру для разметки и почему:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально я выбрала для разметки UD, так как он, как мне показалось, наименее специфичный (а также самый привычный мне...)\n",
    "При приведении тегов к одинаковому набору я упростила сет до:\n",
    "* _NOUN_ (существительные, в т.ч. _PROPN_), \n",
    "* _ADJ_ (краткие и полные прилагательные, причастия), \n",
    "* _VERB_ (глаголы),\n",
    "* _ADP_ (предлоги),\n",
    "* _CONJ_ (как подчинительные, так и сочинительные союзы),\n",
    "* _INTJ_ (междометия),\n",
    "* _PART_ (частицы),\n",
    "* _ADV_ (наречия, деепричастия),\n",
    "* _PRON_ (все виды местоимений),\n",
    "* _X_ (звукоподражания, иностранные куски)\n",
    "\n",
    "Это было сделано с той целью, что, в первую очередь, мы хотим не просто точность для отдельных сложных слов, а общую способность теггера справиться с текстом из неологизмов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я отношу причастия к прилагательным, а деепричастия к наречиям из-за того, что по форме они напоминают друг друга. В условиях текста, где все слова неологизмы, это, как мне кажется, самое важное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pus'ki_b'atyje.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenized = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Три POS теггера для русского языка:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Stanza__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_text = nlp(text)\n",
    "with open(\"stanza_pus'ki_b'atyje.txt\", 'w', encoding='utf-8') as stanza_file:\n",
    "    for sent in stanza_text.sentences:\n",
    "        for word in sent.words:\n",
    "            stanza_file.write(f'{word.text}\\t{word.upos}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __PyMorphy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pymorphy_pus'ki_b'atyje.txt\", 'w', encoding='utf-8') as pymorphy_file:\n",
    "    for word in text_tokenized:\n",
    "        pymorphy_file.write(f'{word}\\t{morph.parse(word)[0].tag.POS}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __MyStem__ \n",
    "\n",
    "(господи, какой он медленный, зашто)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mystem_pus'ki_b'atyje.txt\", 'w', encoding='utf-8') as mystem_file:\n",
    "    for word in text_tokenized:\n",
    "        stems = m.analyze(word)\n",
    "        for item in stems:\n",
    "            if 'analysis' in item:\n",
    "                if item['analysis'] and 'gr' in item['analysis'][0]:\n",
    "                    mystem_file.write(f\"{item['text']}\\t{item['analysis'][0]['gr'].split(',')[0].split('=')[0]}\\n\")\n",
    "                else:\n",
    "                    mystem_file.write(f\"{item['text']}\\tX\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь я, как раз, смотрела, какие вообще встречаются теги в каждом из наборов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def what_are_your_tags(name):\n",
    "    pos_tags = []\n",
    "    with open(name, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if len(line.split('\\t')) > 1:\n",
    "                tag = line.split('\\t')[1]\n",
    "                if tag not in pos_tags:\n",
    "                    pos_tags.append(tag)\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag_manual = what_are_your_tags(\"processed_pus'ki_b'atyje.txt\")\n",
    "pos_tag_stanza = what_are_your_tags(\"stanza_pus'ki_b'atyje.txt\")\n",
    "pos_tag_pymorphy = what_are_your_tags(\"pymorphy_pus'ki_b'atyje.txt\")\n",
    "pos_tag_mystem = what_are_your_tags(\"mystem_pus'ki_b'atyje.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встретившиеся теги для каждого из POS теггеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual POS tags: ['NOUN', 'ADJ', 'NUM', 'PUNCT', 'VERB', 'PROPN', 'ADP', 'CCONJ', 'INTJ', 'PART', 'ADV', 'PRON', 'X', 'SCONJ', 'AUX', 'SYM']\n",
      "Stanza POS tags: ['NOUN', 'ADJ', 'NUM', 'PUNCT', 'PROPN', 'ADP', 'CCONJ', 'VERB', 'INTJ', 'PART', 'SCONJ', 'ADV', 'X', 'AUX', 'SYM', 'PRON']\n",
      "PyMorphy POS tags: ['NOUN', 'ADJF', 'None', 'VERB', 'PREP', 'CONJ', 'ADJS', 'PRCL', 'PRTF', 'INFN', 'INTJ', 'ADVB', 'GRND', 'PRTS']\n",
      "MyStem POS tags: ['S', 'V', 'PR', 'CONJ', 'PART', 'X', 'A', 'ADVPRO', 'INTJ', 'ADV', 'SPRO', 'APRO']\n"
     ]
    }
   ],
   "source": [
    "print(f'Manual POS tags: {pos_tag_manual}\\nStanza POS tags: {pos_tag_stanza}\\nPyMorphy POS tags: {pos_tag_pymorphy}\\nMyStem POS tags: {pos_tag_mystem}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cведение к единому стандарту\n",
    "\n",
    "_См. про упрощение тегов выше:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Stanza и размеченный вручную__ (оба опираются на Universal Dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_ud(text):\n",
    "    pos_set = []\n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "        if len(line.split('\\t')) > 1:\n",
    "            word = line.split('\\t')[0].strip(punctuation)\n",
    "            tag = line.split('\\t')[1]\n",
    "            if tag == 'PROPN':\n",
    "                tag = 'NOUN'\n",
    "            elif tag.endswith('CONJ'):\n",
    "                tag = 'CONJ'\n",
    "            elif tag == 'PUNCT':\n",
    "                continue\n",
    "            elif tag == 'SYM':\n",
    "                continue\n",
    "            elif tag == 'AUX':\n",
    "                tag = 'VERB'\n",
    "            if word and not word.isdigit():\n",
    "                pos_set.append([word, tag])\n",
    "    return pos_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __PyMorphy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pymorphy(text):\n",
    "    pos_set = []\n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "        if len(line.split('\\t')) > 1:\n",
    "            word = line.split('\\t')[0].strip(punctuation)\n",
    "            tag = line.split('\\t')[1]\n",
    "            if tag.startswith('ADJ') or tag.startswith('PRT'):\n",
    "                tag = 'ADJ'\n",
    "            elif tag == 'ADVB' or tag == 'GRND':\n",
    "                tag = 'ADV'\n",
    "            elif tag == 'INFN':\n",
    "                tag = 'VERB'\n",
    "            elif tag == 'None':\n",
    "                tag = 'X'\n",
    "            elif tag == 'PREP':\n",
    "                tag = 'ADP'\n",
    "            elif tag == 'PRCL':\n",
    "                tag = 'PART'\n",
    "            if word and not word.isdigit():\n",
    "                pos_set.append([word, tag])\n",
    "    return pos_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __MyStem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_mystem(text):\n",
    "    pos_set = []\n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "        if len(line.split('\\t')) > 1:\n",
    "            word = line.split('\\t')[0].strip(punctuation)\n",
    "            tag = line.split('\\t')[1]\n",
    "            if tag.endswith('PRO'):\n",
    "                tag = 'PRON'\n",
    "            elif tag == 'A':\n",
    "                tag = 'ADJ'\n",
    "            elif tag == 'S':\n",
    "                tag = 'NOUN'\n",
    "            elif tag == 'PR':\n",
    "                tag = 'ADP'\n",
    "            elif tag == 'V':\n",
    "                tag = 'VERB'\n",
    "            if word and not word.isdigit():\n",
    "                pos_set.append([word, tag])\n",
    "    return pos_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pymorphy_pus'ki_b'atyje.txt\", encoding='utf-8') as file:\n",
    "    file = file.readlines()\n",
    "    set_pymorphy = from_pymorphy(file)\n",
    "    \n",
    "with open(\"mystem_pus'ki_b'atyje.txt\", encoding='utf-8') as file:\n",
    "    file = file.readlines()\n",
    "    set_mystem = from_mystem(file)\n",
    "\n",
    "with open(\"stanza_pus'ki_b'atyje.txt\", encoding='utf-8') as file:\n",
    "    file = file.readlines()\n",
    "    set_stanza = from_ud(file)\n",
    "    \n",
    "with open(\"processed_pus'ki_b'atyje.txt\", encoding='utf-8') as file:\n",
    "    file = file.readlines()\n",
    "    set_manual = from_ud(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравниваем с размеченным руками эталоном:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_or_right(set_pairs):\n",
    "    if len(set_pairs) > len(set_manual):\n",
    "        bigger_set = set_pairs\n",
    "        set_pairs = set_manual\n",
    "    else:\n",
    "        bigger_set = set_manual\n",
    "    wrong = 0\n",
    "    right = 0\n",
    "    for index, pair in enumerate(set_pairs):\n",
    "        for i in range(index, len(bigger_set)):\n",
    "            if bigger_set[i][0] == pair[0]:\n",
    "                if bigger_set[i][1] == pair[1]:\n",
    "                    right += 1\n",
    "                else:\n",
    "                    wrong += 1\n",
    "                break\n",
    "    return wrong, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pymorphy, right_pymorphy = wrong_or_right(set_pymorphy)\n",
    "wrong_stanza, right_stanza = wrong_or_right(set_stanza)\n",
    "wrong_mystem, right_mystem = wrong_or_right(set_mystem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Считаем _accuracy_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyStem: accuracy = 0.6646586345381527\n",
      "PyMorphy: accuracy = 0.7049626104690686\n",
      "Stanza: accuracy = 0.8448275862068966\n"
     ]
    }
   ],
   "source": [
    "print(f'MyStem: accuracy = {right_mystem / (wrong_mystem + right_mystem)}\\nPyMorphy: accuracy = {right_pymorphy / (wrong_pymorphy + right_pymorphy)}\\nStanza: accuracy = {right_stanza / (wrong_stanza + right_stanza)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всего с задачей справилась **_Stanza_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"good_reviews.txt\", 'r', encoding='utf-8') as file:\n",
    "    text_review_good = file.read()\n",
    "    \n",
    "with open(\"bad_reviews.txt\", 'r', encoding='utf-8') as file:\n",
    "    text_review_bad = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_text_review_good = nlp(text_review_good)\n",
    "stanza_text_review_bad = nlp(text_review_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('good_pos.txt', 'w', encoding='utf-8') as f:\n",
    "    for sent in stanza_text_review_good.sentences:\n",
    "        for word in sent.words:\n",
    "            f.write(f'{word.text}\\t{word.lemma}\\t{word.upos}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bad_pos.txt', 'w', encoding='utf-8') as f:\n",
    "    for sent in stanza_text_review_bad.sentences:\n",
    "        for word in sent.words:\n",
    "            f.write(f'{word.text}\\t{word.lemma}\\t{word.upos}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 шаблона:\n",
    "1. _вообще не, совсем не_ + NOUN / VERB / ADJ → скорее негативное (_вообще не понравилось_, _совсем не перспективный_, _вообще не айс_)\n",
    "2. _вполне_ + ADV / ADJ → скорее положительное (_вполне хорошо_, _вполне достойный_)\n",
    "3. _никакой_ (в разных падежах) + NOUN → дополнительная дистрибуция (_никаких проблем_ vs. _никакой перспективы_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сам чанкер:\n",
    "\n",
    "Из него достаем те n-граммы, которые встречаются только в плохих / хороших ревью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(text_name):\n",
    "    first_pattern = []\n",
    "    second_pattern = []\n",
    "    third_pattern = []\n",
    "    tagged_lines = []\n",
    "    with open(text_name, encoding='utf-8') as tagged_file:\n",
    "        for line in tagged_file:\n",
    "            if len(line.split('\\t')) == 3:\n",
    "                word, lemma, pos_tag = line.strip().split('\\t')\n",
    "                if pos_tag == 'PUNCT':\n",
    "                    continue\n",
    "                tagged_lines.append([word, lemma, pos_tag])\n",
    "    for index, line in enumerate(tagged_lines):\n",
    "        if len(tagged_lines) > index + 2:\n",
    "            word, lemma, pos_tag = line\n",
    "            word_next, lemma_next, pos_tag_next = tagged_lines[index + 1]\n",
    "            word_next_next, lemma_next_next, pos_tag_next_next = tagged_lines[index + 2]\n",
    "            if (word.lower() == 'вообще' or word.lower() == 'совсем') and word_next == 'не' and (pos_tag_next_next in ['NOUN', 'VERB', 'ADJ']):\n",
    "                if lemma + ' ' + lemma_next + ' ' + lemma_next_next not in first_pattern:\n",
    "                    first_pattern.append(lemma + ' ' + lemma_next + ' ' + lemma_next_next)\n",
    "            elif word == 'вполне' and (pos_tag_next == 'ADV' or pos_tag_next == 'ADJ'):\n",
    "                if lemma + ' ' + lemma_next not in second_pattern:\n",
    "                    second_pattern.append(lemma + ' ' + lemma_next)\n",
    "            elif lemma == 'никакой' and pos_tag_next == 'NOUN':\n",
    "                if lemma + ' ' + lemma_next not in third_pattern:\n",
    "                    third_pattern.append(lemma + ' ' + lemma_next)\n",
    "    return set(first_pattern), set(second_pattern), set(third_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_bad_grams = []\n",
    "for i, chunk in enumerate(chunker('bad_pos.txt')):\n",
    "    only_bad_grams.append(chunk.difference(chunker('good_pos.txt')[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_good_grams = []\n",
    "for i, chunk in enumerate(chunker('good_pos.txt')):\n",
    "    only_good_grams.append(chunk.difference(chunker('bad_pos.txt')[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Встраивание функции в программу из предыдущей домашки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.json', encoding='utf-8') as json_file:\n",
    "    db_mts = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(db):  # to divide into bad reviews and good ones\n",
    "    goods = []\n",
    "    bads = []\n",
    "    for item in db:\n",
    "        items = list(map(int, db[item][-1].values()))  # grades into integers\n",
    "        average = sum(items) / len(items)  # average grade for all the criteria\n",
    "        if average > 3:  # more than 3 -> good\n",
    "            text = db_mts[item][0] + '\\n' + db[item][1]\n",
    "            goods.append(text)\n",
    "        elif average < 3:  # less than 3 -> bad\n",
    "            text = db_mts[item][0] + '\\n' + db[item][1]\n",
    "            bads.append(text)\n",
    "    if len(goods) > len(bads):  # make them equal\n",
    "        goods = goods[:len(bads)]\n",
    "    else:\n",
    "        bads = bads[:len(goods)]\n",
    "    with open('good_reviews.txt', 'w', encoding='utf-8') as f_good:\n",
    "        f_good.write('\\n'.join(goods))\n",
    "    with open('bad_reviews.txt', 'w', encoding='utf-8') as f_bad:\n",
    "        f_bad.write('\\n'.join(bads))\n",
    "    return goods, bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker(word):  # checker for digits and punctuation\n",
    "    for char in punctuation:\n",
    "        if char in word:\n",
    "            return False\n",
    "    for char in digits:\n",
    "        if char in word:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):  # tokenizer...\n",
    "    tokenized = []\n",
    "    for part in text:\n",
    "        words = []\n",
    "        tokenized_part = word_tokenize(part)  # tokenize\n",
    "        for word in tokenized_part:\n",
    "            if checker(word):\n",
    "                word = word.lower()  # make lower\n",
    "                word = morph.parse(word)[0].normal_form  # to normal form\n",
    "                words.append(word)\n",
    "        tokenized.append(words)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dic(tokenized_text, good_or_bad):  # frequency dictionary of the tokens\n",
    "    good_tokens = dict()\n",
    "    bad_tokens = dict()\n",
    "    for index, part in enumerate(tokenized_text):\n",
    "        for token in part:\n",
    "            if good_or_bad[index] == 1:\n",
    "                if token not in good_tokens:\n",
    "                    good_tokens[token] = 1\n",
    "                else:\n",
    "                    good_tokens[token] += 1\n",
    "            else:\n",
    "                if token not in bad_tokens:\n",
    "                    bad_tokens[token] = 1\n",
    "                else:\n",
    "                    bad_tokens[token] += 1\n",
    "    good_tokens = dict(sorted(good_tokens.items(), key=lambda item: item[1], reverse=True))  # good dictionary sorting\n",
    "    bad_tokens = dict(sorted(bad_tokens.items(), key=lambda item: item[1], reverse=True))  # bad dictionary sorting\n",
    "    good_vals = set(good_tokens.keys())  # all the good tokens\n",
    "    bad_vals = set(bad_tokens.keys())  # # all the bad tokens\n",
    "    return good_tokens, bad_tokens, good_vals, bad_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_small(words, dictionary):  # delete non-frequent tokens\n",
    "    real_words = []\n",
    "    for word in words:\n",
    "        if dictionary[word] > 2:  # occurrence is more than 2\n",
    "            real_words.append(word)\n",
    "    return real_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews, bad_reviews = splitter(db_mts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = good_reviews + bad_reviews\n",
    "y = np.concatenate((np.full((len(good_reviews),), 1), np.full((len(bad_reviews),), 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_dic, bad_dic, good_words, bad_words = create_dic(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_good = good_words.difference(bad_words)\n",
    "only_bad = bad_words.difference(good_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "really_bad = del_small(only_bad, bad_dic)\n",
    "really_good = del_small(only_good, good_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Меняем функцию *good_or_bad_checker()* с n-граммами:\n",
    "\n",
    "(я еще добавила веса, так как подумала, что такие сочетания точно больше говорят об отзыве, чем 1 слово)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_or_bad_checker(test, good, bad):\n",
    "    preds = []\n",
    "    for review in test:\n",
    "        for_good = 0\n",
    "        for_bad = 0\n",
    "        for i, token in enumerate(review):\n",
    "            if token in good:\n",
    "                for_good += 1\n",
    "            elif token in bad:\n",
    "                for_bad += 1\n",
    "            if i + 1 < len(review):\n",
    "                if token + ' ' + review[i + 1] in only_good_grams[1] or token + ' ' + review[i + 1] in only_good_grams[2]:\n",
    "                    for_good += 2\n",
    "                elif token + ' ' + review[i + 1] in only_bad_grams[1] or token + ' ' + review[i + 1] in only_bad_grams[2]:\n",
    "                    for_bad += 2\n",
    "            if i + 2 < len(review):\n",
    "                if token + ' ' + review[i + 1] + ' ' + review[i + 2] in only_good_grams[0]:\n",
    "                    for_good += 2\n",
    "                elif token + ' ' + review[i + 1] + ' ' + review[i + 2] in only_bad_grams[0]:\n",
    "                    for_bad += 2\n",
    "        if for_good > for_bad:\n",
    "            preds.append(1)\n",
    "        elif for_bad > for_good:\n",
    "            preds.append(0)\n",
    "        else:\n",
    "            preds.append(random.randint(0, 1))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Считаем _accuracy_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_preds = good_or_bad_checker(X_test, really_good, really_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7589285714285714"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этот раз качество **лучше почти на .02** (по сравнению с 0.7410714285714286 из прошлого ДЗ)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
